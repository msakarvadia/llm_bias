import sys
sys.path.append('../')
from utils import load_quantized_model_and_tokenizer, assess_device_memory
from data.preprocess_data import load_data_from_csv
import os.path
import pandas as pd

from transformers import LlamaConfig
import torch
from torch import nn
from typing import Optional, Tuple
import warnings
from transformers import AutoTokenizer, LlamaForCausalLM

from transformers.models.llama.modeling_llama import (LlamaFlashAttention2, 
                                                      apply_rotary_pos_emb,
                                                      repeat_kv,
                                                      #LlamaSdpaAttention, 
                                                      LlamaAttention,
                                                      LlamaMLP,
                                                      LlamaRMSNorm )

from transformers.cache_utils import Cache, DynamicCache

models = [
"meta-llama/Llama-2-7b-hf",
#"meta-llama/Llama-2-70b-chat-hf",
#"meta-llama/Llama-2-70b-hf",
#"meta-llama/Llama-2-7b-chat-hf",
#"meta-llama/Llama-2-13b-chat-hf",
#"meta-llama/Llama-2-13b-hf",
]


class LlamaSdpaAttention(LlamaAttention):
    """
    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from
    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to
    SDPA API.
    """

    # Adapted from LlamaAttention.forward
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        if output_attentions:
            # TODO: Improve this warning with e.g. `model.config.attn_implementation = "manual"` once this is implemented.
            logger.warning_once(
                "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, "
                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
            )
            return super().forward(
                hidden_states=hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )

        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                )

        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,
        # Reference: https://github.com/pytorch/pytorch/issues/112577.
        if query_states.device.type == "cuda" and attention_mask is not None:
            query_states = query_states.contiguous()
            key_states = key_states.contiguous()
            value_states = value_states.contiguous()

        attn_output = torch.nn.functional.scaled_dot_product_attention(
            query_states,
            key_states,
            value_states,
            attn_mask=attention_mask,
            dropout_p=self.attention_dropout if self.training else 0.0,
            # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.
            is_causal=self.is_causal and attention_mask is None and q_len > 1,
        )
        print(attn_output.shape)

        attn_output = attn_output.transpose(1, 2).contiguous()
        print(attn_output.shape)
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        print(attn_output.shape)

        attn_output = self.o_proj(attn_output)

        print("HERE")
        print(attn_output.shape)
        return attn_output, None, past_key_value

LLAMA_ATTENTION_CLASSES = {
    "eager": LlamaAttention,
    "flash_attention_2": LlamaFlashAttention2,
    "sdpa": LlamaSdpaAttention,
}

"""LLamaDecoderLayer"""
class LlamaDecoderLayer_Modified(nn.Module):
    def __init__(self, config: LlamaConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)

        self.mlp = LlamaMLP(config)
        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.attn_activations = None

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        **kwargs,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*):
                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,
                query_sequence_length, key_sequence_length)` if default attention is used.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
        """
        if "padding_mask" in kwargs:
            warnings.warn(
                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
            )

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            **kwargs,
        )

        self.attn_activations = hidden_states
        print("Dim of attn activations: ", self.attn_activations.shape)

        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs

if __name__=="__main__":

    #iterate over models
    for m in models:
        model_name = m

        #Load model, tokenizer
        #model, tokenizer = load_quantized_model_and_tokenizer(model_name)

        model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
        assess_device_memory()


        inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
        outputs = model(**inputs, output_hidden_states=True)
        print("did inference part 1")

        # Assign new decoder layer:
        layer_idx = 3
        print(type(model.model.layers[layer_idx]))
        #model.model.layers[layer_idx] = LlamaDecoderLayer_Modified(model.config, layer_idx)
        model.model.layers[layer_idx].self_attn = LlamaSdpaAttention(model.config, layer_idx)
        print("Did assing new layer")

        outputs = model(**inputs)
        print(model.config)
       
        #print(outputs['hidden_states'][layer_idx].shape) # This is outputing residual stream at given layer
        #print("model functions:", dir(model.model.layers[layer_idx].self_attn))
        #print("model variables:", vars(model.model.layers[layer_idx].self_attn.forward)) #This is the function that returns a layers Attention hidden states
        #print(outputs['attentions'][layer_idx].shape) #This is the key, value attention heat map


        '''
        #add column to csv for model name, append generated text to that column
        for index, row in data.iterrows():
            #skip rows that already have content
            if not pd.isna(results.loc[index, model_name]):
                continue
            prompt = row['prompt']
            #better understand what layer_idx = 3 generate text produces TODO
            text = generate_text(text=prompt, model=model, tokenizer=tokenizer)
            results.loc[index, model_name] = text
            if index%50 == 0:
                print(results.tail())
                #Save results
                save_data_as_csv(df=results, filename=results_file_path)
        '''
